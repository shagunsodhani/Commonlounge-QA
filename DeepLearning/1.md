**Question**

Shagun, I have a two part question! This will probably be quite a lengthy answer, but very helpful to me and probably to many others.

What are the most significant advancements made in Natural Language Processing in the last few years? It would be great if you could give me intuition regarding the importance of the various advancements, and possibly include pointers for further reading.

As a practitioner, wheres the line between machine learning / deep learning for various NLP applications? When should I use the newer DL methods vs traditional ML / NLP methods?

**Answer** 

Let's start with the first part

*What are the most significant advancements made in Natural Language Processing in the last few years? It would be great if you could give me intuition regarding the importance of the various advancements, and possibly include pointers for further reading.*

* Word Embeddings:

    * Development of very efficient, unsupervised algorithms that can project the words into a high-dimensional vector space while capturing the notion of similarity between the words.

    * Models
        * Word2Vec - [Paper](https://arxiv.org/pdf/1301.3781.pdf), [Summary](https://gist.github.com/shagunsodhani/176a283e2c158a75a0a6)
        * Glove - [Paper](https://nlp.stanford.edu/pubs/glove.pdf), [Summary](https://gist.github.com/shagunsodhani/efea5a42d17e0fcf18374df8e3e4b3e8)

    * Word embeddings generated using these models generalise well across different downstream tasks like sentence classification.

* Sentence Embeddings:

    * Once we have the word embeddings, we can use different compositional models to obtain embeddings for the sentences. 

    * Compositional models can include simple things like (wieghted or unweighted) averaging of word vectors and more sophisticated ones like:
        * Skip-Thought Vectors - [Paper](https://arxiv.org/abs/1506.06726), [Summary](https://gist.github.com/shagunsodhani/4a4eb32de8cabf21bda9a4ada15c46e8)
        * Recusrive Neural Tensor Network - [Paper](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf), [Summary](https://gist.github.com/shagunsodhani/6ca136088f58d24f7b08056ec8b97595)

    * These compositional models outperform most of the hand-engineered system for tasks like:
        * measuring semantic relatedness of sentences
        * paraphrase detection
        * Image annotation - Given an image, rank the sentences on basis of how well they describe the image
        * Image search - Given a caption, find the image that is being described.

* Sequence to Sequence Learning:

    * It is essentially an encoder-decoder framework, where given an input sentence, an output sentence is generated.
    
    * Sequence to Sequence Learning with Neural Networks - [Paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), [Summary](https://gist.github.com/shagunsodhani/a2915921d7d0ac5cfd0e379025acfb9f)
    
    * The seq2seq framework has been extended for performing various interesting tasks:

        * Dialogue Systems
            * A Neural Conversational Model
            * Train the seq2seq model using chat data and start conversing with the system
            * [Paper](https://arxiv.org/abs/1506.05869), [Summary](https://gist.github.com/shagunsodhani/ec6835964df0e49fdef0459c8b334b94)
            * There are a lot of shortcomings in this model but the proof of concept is clearly there.
            * This work was followed by other works like
                * A Persona-Based Neural Conversation Model - attempts to add coherency to the responses generate by seq2seq models.
                * [Paper](https://arxiv.org/abs/1603.06155), [Summary](https://gist.github.com/shagunsodhani/8ad464e7d0ea4c7c6ed5189ac4e44095)

        * Deep Math: Deep Sequence Models for Premise Selection
            * Given a mathematical theorem, find the premises which can be used to prove the theorem, without using any hand-engineered features
            * [Paper](https://arxiv.org/abs/1606.04442), [Summary](https://gist.github.com/shagunsodhani/d8387256f2bb08f39509600f9d7db498)

* Neural Machine Translation
    
    * Towards the end of 2016, a [New York Times article](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?_r=0) described how Google used AI to boost the performance of its translation tool by around 60%.
    * [Paper](https://arxiv.org/pdf/1609.08144.pdf) described the encoder-decoder architecture being used.

* Use of attention and memory in language models

    * Memory Networks
        * Proposed the use of a long-term memory component (which acts as a dynamic knowledge base) to maintain context over the longer time duration in the context of question answering.
        * Memory Networks - [Paper](http://arxiv.org/pdf/1410.3916v11.pdf), [Summary](https://gist.github.com/shagunsodhani/c7a03a47b3d709e7c592fa7011b0f33e)
        * End-To-End Memory Networks - [Paper](https://arxiv.org/pdf/1503.08895v5.pdf), [Summary](https://gist.github.com/shagunsodhani/17881da05d9ee1f6539b2baa8067a6ef)

* Machine Comprehension

    * Task of reading a given document and answer questions about it

    * Key-Value Memory Networks:
        * Memory Networks inspired by Knowledge bases which use memory slots of the form (key, value) to encode long and short term context
        * [Paper](https://arxiv.org/abs/1606.03126)
        * [Summary](https://gist.github.com/shagunsodhani/a5e0baa075b4a917c0a69edc575772a8)

    * Query Regression Networks:
        * Similar to Recurrent Neural Network (RNN) which keeps updating the "local query vector" as it reads the document.
        * [Paper](https://arxiv.org/abs/1606.04582)
        * [Summary](https://gist.github.com/shagunsodhani/93caa283af3c151372f4be86ed4c4b99)

* Visual Q&A/Dialogue Systems
    
    * The basic idea is that the model is given an image and then it has to answer questions about the image or carry on a conversation about the image and so on.
    * There are a couple of interesting papers in the area - which I have not gone through completely:
        * [VQA: Visual Question Answering](https://arxiv.org/abs/1505.00468)
        * [Visual Dialog](https://arxiv.org/abs/1611.08669)
        * [End-to-end optimization of goal-driven and visually grounded dialogue systems](https://arxiv.org/abs/1703.05423)
        * [Stacked Attention Networks for Image Question Answering](https://arxiv.org/abs/1511.02274)

    * The interesting aspect is that some of these papers have modelled the problem as that of reinforcement learning so it uses techniques and ideas from all the three areas - computer vision, Natural language processing and reinforcement learning.

**Let's move to the second part.**

*As a practitioner, wheres the line between machine learning / deep learning for various NLP applications? When should I use the newer DL methods vs traditional ML / NLP methods?*

Deep Learning methods often need a lot of supervised data - which is not easy to come by. So many times, you would either hire some people to do the manual annotation of data for you or you would do it yourself. Alternatively, you may go back to the traditional methods and develop your model. Probably you could use traditional methods to develop supervised dataset which you can then use to train your deep network. Or you might come up with an unsupervised solution for the problem (this does not happen generally). Which route you eventually take depends on the constraints of your time and budget, expectations from the system and so on.