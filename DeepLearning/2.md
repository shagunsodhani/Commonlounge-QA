**Question**

What are some areas you think Deep Learning is ill-suited for and why?

**Answer**

Deep Learning has its own shortcomings and is not suitable for areas/domains/problems where these limitations hold:

* **Lack of reasoning ability**
    
    * Deep Learning is very good for tasks like classification but not so good when for tasks like reasoning or inference.
    
    * One approach could be to use energy based models along with deep neural networks. 
    
    * (Yann LeCunn's paper)[https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=20&pagesize=80&citation_for_view=WLN3QrAAAAAJ:8k81kl-MbHgC] on energy-based models and structured prediction explains this idea further.

* **Lack of model interpretation**
    
    * Even though our deep learning model may achieve near perfect accuracy, it is difficult to interpret the model and understand why is behaves in a particular way.
    
    * This lack of interpretation makes it difficult to improve the model and to trust its predictions.
    
    * Further it limits the adoption of deep learning models in areas like finance and medical sciences where models like logistic regression and decision tree are still favoured for their explainability.
    
    * There has been some progress towards visualising which neurons are activated/fired when a certain input is given to the network and use that as a proxy to interpret the network.
    
    * The more recent [Local Interpretable Model-agnostic Explanations aka LIME paper](https://gist.github.com/shagunsodhani/bd744ab6c17a2289ca139ea586d1d65e) aims to explain individual predictions for any classifier model. 

* **Need for a lot of data**
    
    * Deep Learning needs a lot of data, in many cases supervised data, to obtain good results. 
    
    * The good news is that [adversarial training](https://gist.github.com/shagunsodhani/1f9dc0444142be8bd8a7404a226880eb) has emerged as a game changer in the context of unsupervised training.
    
    * [@apsdehal](https://github.com/apsdehal) pointed to Andrew L Beams' blog on [You can probably use deep learning even if your data isn't that big](http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html) which demonstrates that we can get benefits from deep learning using small datasets as well.

* **Fooling the network** 
    
    * It turns out that neural networks can be fooled into misclassifying the input data by making small changes to the input data.
    
    * Refer to [Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images paper](http://anhnguyen.me/project/fooling/) for more details. 

So essentially domains where one needs reasoning ability or where a lot of supervised data is not available easily or where model interpretation is very valuable - these domains are not well suited for deep learning and more traditional machine learning models may be more appropriate.

